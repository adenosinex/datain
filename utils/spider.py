import threading
import time
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import traceback 
# playwright install 

class StealthBrowser:
    def __init__(self, headless=True):
        self.playwright = sync_playwright().start()
        # åˆ›å»ºæµè§ˆå™¨
        self.browser = self.playwright.chromium.launch(
            headless=headless,
            args=["--disable-blink-features=AutomationControlled"]
        )

        # åˆ›å»º Contextï¼ˆåœ¨è¿™é‡Œè®¾ç½® UAã€è¯­è¨€ç­‰ï¼‰
        self.context = self.browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/116.0.0.0 Safari/537.36"
            ),
            locale="zh-CN",
            viewport={"width": 1920, "height": 1080},
            java_script_enabled=True
        )

        # åˆ›å»º Page
        self.page = self.context.new_page()

    def get_html(self, url):
        """è·å–ç½‘é¡µ HTML æºç """
        try:
            self.page.goto(url, timeout=5000)  # 20s è¶…æ—¶
            self.page.wait_for_load_state("networkidle")  # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            return self.page.content()
        except Exception as e:
            print("âŒ è·å– HTML å¤±è´¥:", e)
            traceback.print_exc()
            return self.page.content()

    def get_text(self, url):
        """è·å–ç½‘é¡µçº¯æ–‡æœ¬"""
        html = self.get_html(url)
        if html:
            soup = BeautifulSoup(html, "html.parser")
            return soup.get_text(separator="\n", strip=True)
        return None

    def quit(self):
        """å…³é—­æµè§ˆå™¨"""
        try:
            self.context.close()
            self.browser.close()
            self.playwright.stop()
        except Exception as e:
            print("âŒ å…³é—­æµè§ˆå™¨å¤±è´¥:", e)


def run_spider():
    bot = StealthBrowser(headless=True)

    url = "https://zhuanlan.zhihu.com/p/15865355450"
    from  utils.memorydb import InMemoryURLDB
    db= InMemoryURLDB(r'utils\urlcontent.db')
    cnt=0
    stop_t=0.5
    while True:
        cnt+=stop_t
        url=db.get_url()
        if url:
            text = bot.get_text(url)
            if text:
                print("çº¯æ–‡æœ¬é¢„è§ˆ:", len(text), "...\n")

            db.complete_content(url, text)
        else:
            if int(cnt)%60==0 and cnt>10:
                print(time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),"æ²¡æœ‰å¾…å¤„ç†çš„ URLï¼Œç­‰å¾…5ç§’...")
        time.sleep(stop_t)
    bot.quit()

# çº¿ç¨‹æ§åˆ¶
refresh_thread = None
stop_event = threading.Event()

def start_spider():
    """å¯åŠ¨åå°åˆ·æ–°çº¿ç¨‹"""
    global refresh_thread
    if refresh_thread and refresh_thread.is_alive():
        print("ğŸ”„ spiderçº¿ç¨‹å·²åœ¨è¿è¡Œ")
        return

    stop_event.clear()
    refresh_thread = threading.Thread(target=run_spider, daemon=True)
    refresh_thread.start()
    print("ğŸš€ å·²å¯åŠ¨ spider è‡ªåŠ¨åˆ·æ–°å®ˆæŠ¤çº¿ç¨‹")

# =======================
# ä½¿ç”¨ç¤ºä¾‹
# =======================
if __name__ == "__main__":
    run_spider()
